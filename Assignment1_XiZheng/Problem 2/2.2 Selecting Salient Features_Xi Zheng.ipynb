{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize, sent_tokenize\n",
    "import operator\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xizheng/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WSD/wsd_data.xml\") as f:\n",
    "    unformat_data = f.readlines()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defalt_wsd_data():  #Since I always change the element in wsd_data, I defalt it when building every feature\n",
    "    wsd_data = {}\n",
    "    for i in range(len(unformat_data)):\n",
    "        if unformat_data[i].startswith('<welt') == True:\n",
    "            wsd_data[unformat_data[i][:-1]] = []\n",
    "        else:\n",
    "            if unformat_data[i] != '\\n':\n",
    "                     wsd_data[list(wsd_data)[len(wsd_data)-1]].append(unformat_data[i][:-1]) \n",
    "                #Make wsd_data becomes a dictionary, each word contains their own instance\n",
    "    return wsd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_feature(dic,wsd_data): # Same as above\n",
    "    #Simplify_wsd_BOWfeature = {}\n",
    "\n",
    "    for i in wsd_data.keys():    \n",
    "        big_list = []\n",
    "        big_list_simplify = []\n",
    "        for j in wsd_data[i]:\n",
    "            if j == '</welt>':\n",
    "                wsd_data[i] = big_list\n",
    "                dic[i] = big_list_simplify\n",
    "            \n",
    "                break\n",
    "            if j.startswith('<instance'):\n",
    "                temp = [j]\n",
    "                temp_simplify = []\n",
    "            else:\n",
    "                if j.startswith('<ans'):\n",
    "                    temp_simplify.append(j.split(' ')[2][:-2])\n",
    "                if j.startswith('<') == False:\n",
    "                    temp_simplify.append(j)\n",
    "                if j.startswith('</instance') == False:\n",
    "                    temp.append(j)\n",
    "                else:\n",
    "                    temp.append(j)\n",
    "                    big_list.append(temp)\n",
    "                    big_list_simplify.append(temp_simplify)\n",
    "    return dic\n",
    "#Improve wsd_data dictionary, each instance belongs to their own list, and all the instances that \n",
    "#contains one welt belongs a big list, which is the value of the key welt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenit(text):\n",
    "    text_token = wordpunct_tokenize(text)\n",
    "    return text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(wsd_dic):\n",
    "    vocabulary_dic = {}\n",
    "    for i in wsd_dic.keys():\n",
    "        word_count = {}\n",
    "        updated_word_count = {}\n",
    "        for j in wsd_dic[i]:\n",
    "            contex = j[-1].replace('<head>', '').replace('</head>', '')\n",
    "            \n",
    "            tokened_text = tokenit(contex)\n",
    "            for x in tokened_text:\n",
    "                if x in word_count:\n",
    "                    word_count[x] += 1\n",
    "                else:\n",
    "                    word_count[x] = 1\n",
    "        for k in word_count:\n",
    "            if word_count[k] >= 20: # I choose words that appears more than 20 times in all context for one head word\n",
    "                                    # to be in my vocabulary for that word\n",
    "                updated_word_count[k] = word_count[k]\n",
    "        vocabulary_dic[i] = updated_word_count\n",
    "    return vocabulary_dic\n",
    "\n",
    "def bag_of_words_feature(text_token, one_word_vocabulary_dic):\n",
    "    \n",
    "    vector=[]\n",
    "    for i in one_word_vocabulary_dic.keys():\n",
    "        if i in text_token:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simplify_wsd_BOWfeature = simplify_feature({}, defalt_wsd_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Head_Word_Vocabulary = vocabulary(Simplify_wsd_BOWfeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "for welt, attribute in Simplify_wsd_BOWfeature.items(): \n",
    "    \n",
    "    for i in attribute:\n",
    "        \n",
    "        context = i[-1].replace('<head>', '').replace('</head>', '') # abandon head tag\n",
    "        tokened_context = tokenit(context)\n",
    "        bow_vector = bag_of_words_feature(tokened_context,Head_Word_Vocabulary[welt])\n",
    "        Feature_Selection_X\n",
    "        i.append(bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.1_Bag_of_Words.txt', 'w') as f:\n",
    "    for i, j in Simplify_wsd_BOWfeature.items():\n",
    "        f.writelines('{' +i+ ': \\n')\n",
    "        f.writelines(str(j) + '}\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.1_Bag_of_Words_vocabulary.txt', 'w') as f:\n",
    "    for i, j in Head_Word_Vocabulary.items():\n",
    "        f.writelines('{' +i+ ': \\n')\n",
    "        f.writelines(str(j) + '}\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collocation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simplify_wsd_CollocationFeature = simplify_feature({}, defalt_wsd_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "for welt, attribute in Simplify_wsd_CollocationFeature.items(): \n",
    "    for i in attribute:\n",
    "        context = i[-1] # abandon head tag\n",
    "        tokened_context =context.split(' ')\n",
    "        off_empty = []\n",
    "        for m in tokened_context:\n",
    "            if m != '':\n",
    "                off_empty.append(m)\n",
    "        #print(off_empty)\n",
    "        \n",
    "        for k in off_empty:\n",
    "            if '<head>' in k:\n",
    "                ind = off_empty.index(k)\n",
    "                Wi_2 = off_empty[ind-2]\n",
    "                Wi_1 = off_empty[ind-1]\n",
    "                Wi1 = off_empty[ind+1]\n",
    "                Wi2 = off_empty[ind+2]\n",
    "                #print([Wi_2, Wi_1, Wi1, Wi2])\n",
    "                POS_tag = nltk.pos_tag([Wi_2, Wi_1, Wi1, Wi2])\n",
    "        \n",
    "        i.append([POS_tag[0][0],POS_tag[0][1],POS_tag[1][0],POS_tag[1][1],POS_tag[2][0],POS_tag[2][1]\n",
    "                  ,POS_tag[3][0],POS_tag[3][1], Wi_1+'_' + Wi_2, Wi1+'_'+Wi2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.1_Collocation.txt', 'w') as f:\n",
    "    for i, j in Simplify_wsd_CollocationFeature.items():\n",
    "        f.writelines('{' +i+ ': \\n')\n",
    "        f.writelines(str(j) + '}\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_word_format(wsd_dic):\n",
    "    format_dic = {}\n",
    "    for i in wsd_dic.keys():\n",
    "        format_set = set()\n",
    "        \n",
    "        for j in wsd_dic[i]:\n",
    "            \n",
    "            context = j[-1] # abandon head tag\n",
    "            tokened_context =context.split(' ')\n",
    "            \n",
    "            for x in tokened_context:\n",
    "                \n",
    "                if '<head>' in x:\n",
    "                    format_set.add(x)\n",
    "        format_list = list(format_set)\n",
    "        format_dic[i] = format_list\n",
    "    return format_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simplify_wsd_NewFeature = simplify_feature({}, defalt_wsd_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "Format_vocabulary = head_word_format(Simplify_wsd_NewFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "for welt, attribute in Simplify_wsd_NewFeature.items(): \n",
    "    for i in attribute:\n",
    "        context = i[-1] # abandon head tag\n",
    "        tokened_context =context.split(' ')\n",
    "        format_vector = []\n",
    "        \n",
    "        for k in tokened_context:\n",
    "            if '<head>' in k:\n",
    "                for j in Format_vocabulary[welt]:\n",
    "                    if k == j:\n",
    "                        format_vector.append(1)\n",
    "                    else:\n",
    "                        format_vector.append(0)\n",
    "                break\n",
    "        i.append(format_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.1_MyNewFeature.txt', 'w') as f:\n",
    "    for i, j in Simplify_wsd_NewFeature.items():\n",
    "        f.writelines('{' +i+ ': \\n')\n",
    "        f.writelines(str(j) + '}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.1_MyNewFeatureVocabulary.txt', 'w') as f:\n",
    "    for i, j in Format_vocabulary.items():\n",
    "        f.writelines('{' +i+ ': \\n')\n",
    "        f.writelines(str(j) + '}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before feature selections, I need to transform collocational feature to numbers instead of word strings and tag strings because I can't use strings to run through SelectKBest function. I decide to use the count of each collocation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_Simplify_wsd_CollocationFeature = {}\n",
    "collo_vocabulary = {}\n",
    "for i in Simplify_wsd_CollocationFeature.keys():\n",
    "    for j in Simplify_wsd_CollocationFeature[i]:\n",
    "        collo_f = j[-1]\n",
    "        for k in collo_f:\n",
    "            if k in collo_vocabulary:\n",
    "                collo_vocabulary[k] +=1\n",
    "            else:\n",
    "                collo_vocabulary[k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Simplify_wsd_CollocationFeature.keys():\n",
    "    collocation_numbers = []\n",
    "    for j in Simplify_wsd_CollocationFeature[x]:\n",
    "        collo_f = j[-1]\n",
    "        empty = []\n",
    "        for k in collo_f:\n",
    "            empty.append(collo_vocabulary[k])\n",
    "        collocation_numbers.append(empty)\n",
    "    Updated_Simplify_wsd_CollocationFeature[x] = collocation_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all three features to one vector\n",
    "Feature_Selection = {}\n",
    "for i in Simplify_wsd_BOWfeature.keys():\n",
    "    senseid = []\n",
    "    senseid_corresponding_feature = []\n",
    "    \n",
    "    for j in range(len(Simplify_wsd_BOWfeature[i])):\n",
    "        \n",
    "        for k in range(len(Simplify_wsd_CollocationFeature[i][j])):\n",
    "            \n",
    "            if 'senseid=' in Simplify_wsd_CollocationFeature[i][j][k]:\n",
    "                \n",
    "                senseid.append(Simplify_wsd_CollocationFeature[i][j][k])\n",
    "                \n",
    "                Bow_f = Simplify_wsd_BOWfeature[i][j][-1]\n",
    "                Collection_f = Updated_Simplify_wsd_CollocationFeature[i][j]\n",
    "                New_f = Simplify_wsd_NewFeature[i][j][-1]\n",
    "                \n",
    "                senseid_corresponding_feature.append(Bow_f + Collection_f + New_f)\n",
    "    Feature_Selection[i] = [senseid_corresponding_feature, np.asarray(senseid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run through SelectKBest, using get_support function to return the index of features that are selected.\n",
    "feature_selection_dic = {}\n",
    "for i in Feature_Selection.keys():\n",
    "    X, y = Feature_Selection[i][0],Feature_Selection[i][1]\n",
    "    selector = SelectKBest(chi2, k=10)\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    X_new = selector.transform(X)\n",
    "\n",
    "    feature_selection_dic[i] = selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2.2.2_FeatureSelection.txt', 'w') as f:\n",
    "    for i, j in feature_selection_dic.items():\n",
    "        f.writelines(i+ ': '+ str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
