{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import operator\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import statistics \n",
    "from statistics import mean\n",
    "from functools import reduce\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "#import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('brown_train') #get all file names under the training dataset\n",
    "#all_text = []\n",
    "all_text2 = ''\n",
    "for file in files: \n",
    "    f = open('brown_train/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    #all_text.append(article.lower()) # lowercase all the words\n",
    "    all_text2 = all_text2 + article.lower()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "    temp = text.split()\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def split_word_tag(text_token):\n",
    "    splited = []\n",
    "    for j in text_token:\n",
    "        temptemp = j.split('/')\n",
    "        splited.append(temptemp)\n",
    "    return splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_START_END(word_tag_list):\n",
    "    \n",
    "    for i in range(len(word_tag_list)):\n",
    "        if i == 0:\n",
    "            word_tag_list[i].append('<START>')\n",
    "        if i != len(word_tag_list)-1:\n",
    "            if '.' in word_tag_list[i][1]:\n",
    "                word_tag_list[i].append('<END>')\n",
    "                if '.' not in word_tag_list[i+1][1]:\n",
    "                    word_tag_list[i+1].append('<START>')\n",
    "            else:\n",
    "                if '<START>' not in word_tag_list[i] and '<END>' not in word_tag_list[i]:\n",
    "                    word_tag_list[i].append('<MID>')\n",
    "        if i == len(word_tag_list)-1:\n",
    "            word_tag_list[i].append('<END>')\n",
    "    return word_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_word_tag = token(all_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_UNK(word_tag_list, threshold):\n",
    "    dic = {}\n",
    "    for i in word_tag_list:\n",
    "        if i[0] in dic:\n",
    "            dic[i[0]] = dic[i[0]] + 1\n",
    "        else:\n",
    "            dic[i[0]] = 1\n",
    "    \n",
    "    for j in word_tag_list:\n",
    "        if dic[j[0]]<threshold:\n",
    "            j[0] = 'UNK'\n",
    "    return word_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_word_tag = split_word_tag(tokened_word_tag)\n",
    "Replaced_word_tag = replace_UNK(pre_word_tag, 4) # Set threshold to 4 to replace words with too small frequencies\n",
    "training_word_tag = add_START_END(Replaced_word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tag={}\n",
    "tag_uni={}\n",
    "tag_bi = {}\n",
    "for i  in range(len(training_word_tag)):\n",
    "    \n",
    "    word_tag_ele = training_word_tag[i][0] + '|'+training_word_tag[i][1]\n",
    "    if word_tag_ele in word_tag:\n",
    "        word_tag[word_tag_ele] += 1\n",
    "    if word_tag_ele not in word_tag:\n",
    "        word_tag[word_tag_ele] = 1\n",
    "        \n",
    "    tag_uni_ele = training_word_tag[i][1]\n",
    "    if tag_uni_ele in tag_uni:\n",
    "        tag_uni[tag_uni_ele] +=1\n",
    "    if tag_uni_ele not in tag_uni:\n",
    "        tag_uni[tag_uni_ele] =1\n",
    "\n",
    "    if i != 0:\n",
    "        tag_bi_ele = training_word_tag[i][1] + '|' + training_word_tag[i-1][1] #ti, ti-1 form\n",
    "        if tag_bi_ele in tag_bi:\n",
    "            tag_bi[tag_bi_ele] +=1\n",
    "        else:\n",
    "            tag_bi[tag_bi_ele] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.1_word-tag counts.txt', 'w') as f:\n",
    "    for i, j in word_tag.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.1_tag-unigram counts.txt', 'w') as f:\n",
    "    for i, j in tag_uni.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.1_tag-bigram counts.txt', 'w') as f:\n",
    "    f.writelines('The tag-bigram has the form ti|ti-1\\n')\n",
    "    for i, j in tag_bi.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probability(tag_unigram,tag_bigram,tag_bigram_prob):\n",
    "    for i in tag_bigram.keys():\n",
    "        ti_1 = (i.split('|'))[1]\n",
    "        tag_bigram_prob[i] = tag_bigram[i]/tag_uni[ti_1]\n",
    "    return tag_bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = transition_probability(tag_uni, tag_bi, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.2_transition probability.txt', 'w') as f:\n",
    "    f.writelines('The tag-bigram has the form ti|ti-1, but it calculate P(tiâˆ’1, ti).\\n')\n",
    "    for i, j in transition_probabilities.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_probability(tag_unigram,word_tag_dic,word_tag_prob):\n",
    "    for i in word_tag_dic.keys():\n",
    "        ti = (i.split('|'))[1]\n",
    "        word_tag_prob[i] = word_tag_dic[i]/tag_uni[ti]\n",
    "    return word_tag_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probabilities = emission_probability(tag_uni, word_tag, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.3_emission probability.txt', 'w') as f:\n",
    "\n",
    "    for i, j in emission_probabilities.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_word = {}\n",
    "Start_tag = {}\n",
    "for i in range(len(training_word_tag)):\n",
    "    if training_word_tag[i][-1]== '<START>':\n",
    "        if training_word_tag[i][0]+'|'+training_word_tag[i][1] in Start_word:\n",
    "            Start_word[training_word_tag[i][0]+'|'+training_word_tag[i][1]] += 1\n",
    "        if training_word_tag[i][0]+'|'+training_word_tag[i][1] not in Start_word:\n",
    "            Start_word[training_word_tag[i][0]+'|'+training_word_tag[i][1]] = 1\n",
    "        \n",
    "        if training_word_tag[i][1] in Start_tag:\n",
    "            Start_tag[training_word_tag[i][1]] +=1\n",
    "        if training_word_tag[i][1] not in Start_tag:\n",
    "            Start_tag[training_word_tag[i][1]] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_tag_prob = {}\n",
    "    \n",
    "for n in Start_tag.keys():\n",
    "    Start_tag_prob[n] = Start_tag[n]/sum(list(Start_tag.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_word_emission_prob = emission_probability(Start_tag, Start_word, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_and_end_words(first_tag, transition_probability, emission_probability, max_iter, P_begin):\n",
    "    previous_tag = first_tag\n",
    "    WORD_output = []\n",
    "    iter_ = 0\n",
    "    while iter_ <= max_iter: #I don't want the sentence be to long who'll have a really small probability, therefore I\n",
    "                             # set an max_iteration.\n",
    "        candidate_tags = {}\n",
    "        for i in transition_probabilities.keys():\n",
    "            if i.split('|')[1] == previous_tag:\n",
    "                candidate_tags[i.split('|')[0]] = transition_probabilities[i]\n",
    "        tag_nomalizer = sum(list(candidate_tags.values()))\n",
    "        TAG = np.random.choice(list(candidate_tags.keys()), 1, \n",
    "                               [k/tag_nomalizer for k in list(candidate_tags.values())])\n",
    "        #print(TAG)\n",
    "        candidate_word_tag = {}\n",
    "        for j in emission_probability.keys():\n",
    "            if j.split('|')[1] == TAG[0]:\n",
    "                if j.split('|')[0] != '':\n",
    "                    candidate_word_tag[j] = emission_probability[j]\n",
    "        word_normalizer = sum(list(candidate_word_tag.values()))\n",
    "        WORD = np.random.choice(list(candidate_word_tag.keys()), 1,\n",
    "                                [l/word_normalizer for l in list(candidate_word_tag.values())])\n",
    "        WORD_output.append(WORD[0])\n",
    "        #print(TAG[0]+ '|') #+ previous_tag])\n",
    "        P_begin += np.log(transition_probability[TAG[0] + '|'+ previous_tag]) + np.log(emission_probability[WORD[0]])\n",
    "        \n",
    "        previous_tag = TAG[0]\n",
    "        if TAG == '.':\n",
    "            break\n",
    "        iter_ += 1\n",
    "    return WORD_output, np.e ** P_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is ['he|pps', '29th|od', 'last|ap', 'whoever|wps', \"she's|pps+bez\", 'right|nn-tl', 'up|rp', ':|:', \"he's|pps+bez\", 'such|abl', 'kennings|nns', ')|)', \"'|'\", \"don't|do*\", ';|.'] with probability 1.4829323916154647e-58\n",
      "\n",
      "The sentence is ['naturally|ql-hl', 'little|ap-hl', 'to|to-hl', 'be|be-hl', 'larger|jjr-hl', '.|.-hl', 'those|dts', 'whatever|wdt', 'oneself|ppl', '``|``', \"where'd|wrb+md\", 'ye|ppss', ':|.'] with probability 9.22193382424271e-43\n",
      "\n",
      "The sentence is ['no|at', 'wednesday|nr', 'even|ql', 'apparently|rb', 'UNK|bed', 'mayor|nn-tl', 'gables|nps', 'any|dti', 'e|nn-tl', '400|cd', 'UNK|abn', 'ought|md', \"let's|vb+ppo\", 'adventure|vb', 'we|ppss-tl', 'may|md-tl', 'have|hv-tl', 'him|ppo-tl', 'commissions|nns-tl', 'beside|in', \"who's|wps+bez-tl\", 'what|wps-tl', 'writes|vbz-tl', ',|,', 'this|dt', 'actively|ql', 'UNK|wrb', 'am|bem', 'interferometer|nn', 'most|ql-tl', 'congolese|jj-tl', 'an|at', 'laughed|vbd', 'those|dts', 'were|bed', 'easier|jjr', 'subjectively|ql', 'upper|jjr', '26|cd', '2d|od', 'all|abn', 'which|wdt'] with probability 4.322610937072391e-178\n",
      "\n",
      "The sentence is ['do|do', ';|.'] with probability 6.1243648347980815e-06\n",
      "\n",
      "The sentence is ['our|pp$', 'sixth|od', 'single|ap', '!|.'] with probability 1.0161588810230291e-14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    p_begin = 0\n",
    "    starter_tag = np.random.choice(list(Start_tag_prob.keys()), 1, list(Start_tag_prob.values()))\n",
    "    candidate_starter_word = {}\n",
    "    for j in star_word_emission_prob.keys():\n",
    "        if j.split('|')[1] == starter_tag:\n",
    "            #print(j)\n",
    "            candidate_starter_word[j] = star_word_emission_prob[j]\n",
    "    starter_normalizer = sum(list(candidate_starter_word.values()))\n",
    "    starer_word = np.random.choice(list(candidate_starter_word.keys()), 1,\n",
    "                                   [w/starter_normalizer for w in list(candidate_starter_word.values())])\n",
    "    #print(starter_tag)\n",
    "    p_begin = np.log(Start_tag_prob[starter_tag[0]]) + np.log(emission_probabilities[starer_word[0]])\n",
    "    \n",
    "\n",
    "    mid, p_begin = mid_and_end_words(starter_tag[0], transition_probabilities,\n",
    "                                     emission_probabilities, 40, p_begin)\n",
    "    sentence = [starer_word[0]] + mid\n",
    "\n",
    "    print('The sentence is ' + str(sentence)+' with probability ' + str(p_begin) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_uniform_probability = {}\n",
    "sum_tag = sum(list(tag_uni.values()))\n",
    "for i in tag_uni:\n",
    "     tag_uniform_probability[i] = tag_uni[i]/sum_tag\n",
    "tag_uniform_probability = dict(sorted(tag_uniform_probability.items(), key=operator.itemgetter(1),reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, all_tag, start_prob, transition_prob, emission_prob):\n",
    "    N = len(all_tag)\n",
    "    T = len(sentence)\n",
    "    path_probability_matrix = np.zeros((N,T))\n",
    "    path = {}\n",
    "    for s in range(N):\n",
    "        # initialize path_probability_matrix\n",
    "        if sentence[0]+'|' + all_tag[s] in emission_prob:\n",
    "            path_probability_matrix[s][0] = start_prob[all_tag[s]] * emission_prob[sentence[0]+'|' + all_tag[s]]\n",
    "        # initialize the path: each tag is possible to begin with\n",
    "        path[all_tag[s]] = [all_tag[s]]\n",
    "        \n",
    "    for t in range(1,T):\n",
    "        \n",
    "        new_tag = {} #The new tag that will become the path\n",
    "        for s in range(N):\n",
    "            candicate_prob = [] # now try the previous tag\n",
    "            for s1 in range(N):\n",
    "                if sentence[t]+'|'+all_tag[s] in emission_prob and all_tag[s]+'|'+all_tag[s1] in transition_prob:\n",
    "                    candicate_prob.append(path_probability_matrix[s1][t-1]\n",
    "                                          *transition_prob[all_tag[s]+'|'+all_tag[s1]]\n",
    "                                          *emission_prob[sentence[t]+'|'+all_tag[s]])\n",
    "                else:\n",
    "                    candicate_prob.append(0)\n",
    "            path_probability_matrix[s][t] = max(candicate_prob) # return the maximum probability\n",
    "            temp_tag = all_tag[np.argmax(candicate_prob)] # this previous tag gives the maximum probability\n",
    "            new_tag[all_tag[s]] = path[temp_tag] + [all_tag[s]] # A new optimal previous tag has been selected\n",
    "        path = new_tag  #ignore all other non-optimal previous tag, only update new one's \n",
    "    \n",
    "    num = np.argmax(path_probability_matrix[:,T-1])\n",
    "    best_tag_path = path[list(path.keys())[num]]\n",
    "    return best_tag_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Test_File.txt', mode = 'r')\n",
    "test_txt = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = {}\n",
    "output_sentence = {}\n",
    "for i in test_txt:\n",
    "    if 'sentence ID' in i:\n",
    "        sentence_id = i[:-1]\n",
    "        tokend_words = []\n",
    "        output_words = []\n",
    "    if '< EOS >' in i:\n",
    "        test_sentence[sentence_id] = tokend_words\n",
    "        output_sentence[sentence_id] = output_words\n",
    "    else: \n",
    "        if 'sentence ID' not in i:\n",
    "            x = i[:-1].lower()\n",
    "            if x not in [k.split(\"|\")[0] for k in list(emission_probabilities.keys())]:\n",
    "                x = 'UNK'\n",
    "            tokend_words.append(x)\n",
    "            output_words.append(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Question_4_4.5.txt', 'w') as f:\n",
    "    for i in test_sentence.keys():\n",
    "        f.writelines(i + '\\n')\n",
    "        tokened_sent = test_sentence[i]\n",
    "        showed_sent = output_sentence[i]\n",
    "        test_tags = viterbi(tokened_sent, list(tag_uniform_probability.keys()),tag_uniform_probability, \n",
    "                            transition_probabilities, emission_probabilities)\n",
    "        for j in range(len(tokened_sent)):\n",
    "            f.writelines(showed_sent[j] + '/'+test_tags[j]+'\\n')\n",
    "        f.writelines('< EOS >\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
