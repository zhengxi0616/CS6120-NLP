{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import operator\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import statistics \n",
    "from statistics import mean\n",
    "from functools import reduce\n",
    "from decimal import Decimal\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('train') #get all file names under the training dataset\n",
    "all_text = []\n",
    "all_text2 = ''\n",
    "for file in files: \n",
    "    f = open('train/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    all_text.append(article.lower()) # lowercase all the words\n",
    "    all_text2 = all_text2 + article.lower()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "    '''\n",
    "    pattern = r ''' '''(?x)    \n",
    "         (?:[a-z]\\.)+\n",
    "         | \\$?\\d+(?:\\.\\d+)?%?\n",
    "         | \\'?\\w+\\'?\n",
    "         | \\<\\S+\\>\n",
    "         | \\w+(?:-\\w+)*\n",
    "         ''' '''\n",
    "    | [][.,;\"'?()~!@#&*+={}/<>:-_`]\n",
    "    pattern = ('\\w+|\\$[\\d\\.]+|\\S+|[\\d\\-]\\w+')\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\(?[\\w\\.]+|\\)?')\n",
    "    '''\n",
    "    text_token = wordpunct_tokenize(text)\n",
    "    return text_token\n",
    "\n",
    "def check_UNK(article, word_dic):\n",
    "    words_list = token(article)\n",
    "    for j in words_list:\n",
    "        if j in word_dic.keys():\n",
    "            word_dic[j] = word_dic[j]+1 #Count the number that each word appears\n",
    "        else:\n",
    "            word_dic[j] = 1 #initiate\n",
    "    return word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_UNK(word_list, word_dic):\n",
    "    for n in range(len(word_list)):\n",
    "        if word_dic[word_list[n]] <= 3:   #Replace wiht UNK if its frequency is too small\n",
    "            word_list[n] = 'UNK'\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sentence(article):\n",
    "    sentence = sent_tokenize(article)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for i in all_text:\n",
    "    word_count = check_UNK(i, word_count) #Update word_count in each iteration\n",
    "\n",
    "tokened_article_to_sentence = [] # A list contains 2 levels of lists\n",
    "for i in all_text:\n",
    "    sentences = token_sentence(i) # token an article to sentences\n",
    "    tokened_article_to_words = [] # A list contains lists of each sentence from the article\n",
    "    for j in sentences:\n",
    "        each_word = token(j) # j is one sentence, token j makes a list of words which form j\n",
    "        temp = replace_UNK(each_word, word_count) # return a wordlist after replacing by 'UNK'\n",
    "    \n",
    "        tokened_article_to_words.append(temp) #add the new wordlist(new sentence with token words) into sentence list \n",
    "    tokened_article_to_sentence.append(tokened_article_to_words) #after containing all sentences, append to article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = []\n",
    "validation_text = []\n",
    "for i in tokened_article_to_sentence:\n",
    "    Sentence_train, Sentence_validation = train_test_split(i, test_size=0.2, random_state=42)\n",
    "    training_text.extend(Sentence_train)\n",
    "    validation_text.extend(Sentence_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram dictionary and probability dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dictionary for unigram\n",
    "def unigram(text_token, unigram_dic):\n",
    "    for i in text_token:   # i is a list of one sentence token\n",
    "        if i not in unigram_dic:\n",
    "            unigram_dic[i] = 1\n",
    "        else:\n",
    "            unigram_dic[i] = unigram_dic[i]+1 #Count the number that each word appears\n",
    "    return unigram_dic\n",
    "\n",
    "\n",
    "#Build a dictionary for unigram's probability, just use simple count(wi)/N\n",
    "def unigram_probability(unigram_dic, length):\n",
    "    unigram_prob_dic = {}\n",
    "    for j in unigram_dic.keys():\n",
    "        unigram_prob_dic[j] = unigram_dic[j]/length\n",
    "    return unigram_prob_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram dictionary and probability dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dictionary for bigram\n",
    "def bigram(text_token, bigram_dic):\n",
    "    for i in range(len(text_token)):\n",
    "        if i != 0:\n",
    "            first_word = text_token[i-1] #Wi-1\n",
    "            second_word = text_token[i] #Wi\n",
    "    \n",
    "            bi_str = second_word + '|' + first_word\n",
    "            if bi_str not in bigram_dic:\n",
    "                bigram_dic[bi_str] = 1\n",
    "            else:\n",
    "                bigram_dic[bi_str] = bigram_dic[bi_str]+1 #Count the number that each bigram appears(count(Wi-1, Wi))\n",
    "    return bigram_dic\n",
    "\n",
    "\n",
    "#Build a dictionary for bigram's probability, use the formulation P(Wi|Wi-1) = count(Wi-1, Wi)/count(Wi-1)\n",
    "def bigram_probability(unigram_dic, bigram_dic):\n",
    "    bigram_prob_dic = {}\n",
    "    for j in bigram_dic.keys():\n",
    "        bigram_words = j.split('|') #in j, after split, bigram_words[1] is Wi-1, bigram_words[0] is Wi,\n",
    "        bigram_prob_dic[j] = bigram_dic[j]/unigram_dic[bigram_words[1]]\n",
    "    return bigram_prob_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram dictionary and probability dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram(text_token,trigram_dic):\n",
    "    if len(text_token)>=2:\n",
    "        for i in range(len(text_token)):\n",
    "            if i!= 0 and i!=1:\n",
    "                first_word = text_token[i-2]\n",
    "                second_word = text_token[i-1]\n",
    "                third_word = text_token[i]\n",
    "                tri_str = third_word + '|' +first_word + '||' + second_word\n",
    "                if tri_str not in trigram_dic:\n",
    "                    trigram_dic[tri_str] = 1\n",
    "                else:\n",
    "                    trigram_dic[tri_str] = trigram_dic[tri_str]+1 #Count the number that each word appears\n",
    "    return trigram_dic\n",
    "\n",
    "def trigram_probability(bigram_dic, trigram_dic):\n",
    "    trigram_prob_dic = {}\n",
    "    for j in trigram_dic.keys():\n",
    "        trigram_words = j.split('|',1) #in j, after split, trigram_words[1] is Wi-2||Wi-1, bigram_words[0] is Wi\n",
    "        trigram_prob_dic[j] = trigram_dic[j]/bigram_dic[trigram_words[1].split('||')[1] + '|' \n",
    "                                                    + trigram_words[1].split('||')[0]]\n",
    "                                        #transfer Wi-2||Wi-1 to Wi|Wi-1 to match the bigram_dic count format\n",
    "        #Probability of trigram = Count(Wi-2, Wi-1, Wi)/Count(Wi-2, Wi-1)\n",
    "        #Count(Wi-2, Wi-1) = bigram_dic[trigram_words[1].split('||')[1] + '|' + trigram_words[1].split('||')[0]]\n",
    "    return trigram_prob_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the unigram, bigram, and trigram wordlevel counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_all_text2 = token(all_text2)\n",
    "tokened_all_text2 = replace_UNK(tokened_all_text2,word_count)  #Replace 'UNK'\n",
    "Unigram_counts = dict(sorted(unigram(tokened_all_text2, {}).items(), key=operator.itemgetter(1),reverse=True))\n",
    "Bigram_counts = dict(sorted(bigram(tokened_all_text2, {}).items(), key=operator.itemgetter(1),reverse=True))\n",
    "Trigram_counts = dict(sorted(trigram(tokened_all_text2, {}).items(), key=operator.itemgetter(1),reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('3.1_ngramCounts.txt', 'w') as f:\n",
    "    f.writelines('A bigram has the form Wi|Wi-1 and a trigram has the form Wi|Wi-2||Wi-1. They were' + \n",
    "                 'the count of (Wi-1, Wi) and (Wi-2, Wi-1, Wi). I write in this form for later conveniencely use.\\n')\n",
    "    for i, j in Unigram_counts.items():\n",
    "        f.writelines(i + ': ' + str(j) + '\\n')\n",
    "    \n",
    "    for k, l in Bigram_counts.items():\n",
    "        f.writelines(k + ': ' + str(l) + '\\n')\n",
    "    for m, n in Trigram_counts.items():\n",
    "        f.writelines(m + ': ' + str(n) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build unigram, bigram and trigram dictionary for training dataset, also calculate the probability for each n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_unigram_dic = {}# Training data's unigram\n",
    "training_bigram_dic = {}\n",
    "training_trigram_dic = {}\n",
    "\n",
    "training_token_length = 0  # Training data contains how many words\n",
    "\n",
    "\n",
    "for j in training_text:\n",
    "    \n",
    "    training_token_length = training_token_length + len(j)\n",
    "    #print(temp_token)\n",
    "    training_unigram_dic = unigram(j, training_unigram_dic)\n",
    "    training_bigram_dic = bigram(j, training_bigram_dic)\n",
    "    training_trigram_dic = trigram(j, training_trigram_dic)\n",
    "\n",
    "\n",
    "training_unigram_dic = dict(sorted(training_unigram_dic.items(), key=operator.itemgetter(1),reverse=True))\n",
    "training_bigram_dic = dict(sorted(training_bigram_dic.items(), key=operator.itemgetter(1),reverse=True))\n",
    "training_trigram_dic = dict(sorted(training_trigram_dic.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_unigram_prob = unigram_probability(training_unigram_dic, training_token_length)\n",
    "training_bigram_prob = bigram_probability(training_unigram_dic, training_bigram_dic)\n",
    "training_trigram_prob = trigram_probability(training_bigram_dic, training_trigram_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_lambdas = []\n",
    "np.random.seed(255)\n",
    "for i in range(0,255):\n",
    "    lam1 = np.random.random_sample()\n",
    "    lam2 = np.random.random_sample()\n",
    "    if lam1 != 0 and lam2!=0:\n",
    "        if lam1 + lam2 < 1:\n",
    "            lam3 = 1- lam1-lam2\n",
    "            possible_lambdas.append([lam1,lam2,lam3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perlexities(text, unigram_prob, bigram_prob, trigram_prob, lambdas):\n",
    "    #lambda_avgperlexity = {}\n",
    "    #for lam in lambdas:\n",
    "    lam1 = lambdas[0]\n",
    "    lam2 = lambdas[1]\n",
    "    lam3 = lambdas[2]\n",
    "    sentence_perlexity = []\n",
    "    P_sentence = 0\n",
    "    word_len = 0\n",
    "    for sentence in text:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "                \n",
    "            unigram_format = sentence[i]\n",
    "            if sentence[i] in unigram_prob:\n",
    "                unigram_format= sentence[i]\n",
    "                \n",
    "            else:\n",
    "                unigram_format = 'UNK'\n",
    "                \n",
    "            p_uni = unigram_prob[unigram_format]        \n",
    "            p_bi = 0\n",
    "            p_tri = 0\n",
    "            \n",
    "            if i == 1:\n",
    "                if sentence[i-1] in unigram_prob:\n",
    "                    bi_i_1 = sentence[i-1]\n",
    "                else:\n",
    "                    bi_i_1 = 'UNK'\n",
    "                bigram_format = unigram_format+'|'+ bi_i_1\n",
    "                if bigram_format in bigram_prob:\n",
    "                    p_bi = bigram_prob[bigram_format]\n",
    "                    \n",
    "            if i != 0 and i !=1:\n",
    "                if sentence[i-1] in unigram_prob:\n",
    "                    bi_i_1 = sentence[i-1]\n",
    "                else:\n",
    "                    bi_i_1 = 'UNK'\n",
    "                \n",
    "                if sentence[i-2] in unigram_prob:\n",
    "                    tri_i_2 = sentence[i-2]\n",
    "                else:\n",
    "                    tri_i_2 = 'UNK'\n",
    "                \n",
    "                bigram_format = unigram_format+'|'+ bi_i_1\n",
    "                trigram_format = unigram_format+'|'+tri_i_2+'||' + bi_i_1\n",
    "                    \n",
    "                if bigram_format in bigram_prob:\n",
    "                    p_bi = bigram_prob[bigram_format]\n",
    "                    \n",
    "                if trigram_format in trigram_prob:\n",
    "                    p_tri = trigram_prob[trigram_format]\n",
    "                    \n",
    "            p_word = lam1*p_uni + lam2*p_bi + lam3*p_tri\n",
    "            \n",
    "            P_sentence = P_sentence + np.log(p_word)  #p_words_in_a_sentence.append(p_word)\n",
    "        word_len = word_len + len(sentence)\n",
    "        \n",
    "    perplexity = pow((np.e),(-1/word_len)*P_sentence)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_avgperlexity = {}\n",
    "for i in possible_lambdas:\n",
    "    lambda_avgperlexity[str(i)] = get_perlexities(validation_text, training_unigram_prob, training_bigram_prob, \n",
    "                training_trigram_prob,i)\n",
    "#dict(sorted(lambda_avgperlexity.items(), key=operator.itemgetter(1),reverse=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0.16461176032353309, 0.5598203068649524, 0.2755679328115145]': 113.68533734764159,\n",
       " '[0.24804604317255252, 0.4205710076505168, 0.3313829491769307]': 113.75439149777728,\n",
       " '[0.20558730582015117, 0.42295571204406324, 0.3714569821357856]': 113.95399371014794,\n",
       " '[0.14635694295559765, 0.4984870392538281, 0.35515601779057426]': 114.56578721648206,\n",
       " '[0.29980414986898307, 0.4452818326708321, 0.2549140174601848]': 114.62403067274991,\n",
       " '[0.1793469896868496, 0.6332920563047046, 0.18736095400844577]': 114.80497852066456,\n",
       " '[0.28928328564226646, 0.5089698609823421, 0.20174685337539144]': 114.95417746157044,\n",
       " '[0.2500554463581307, 0.3642550576103868, 0.3856894960314825]': 114.99603912629117,\n",
       " '[0.1288726154396902, 0.5915665197922657, 0.2795608647680441]': 115.13573231008405,\n",
       " '[0.1617098614852347, 0.656512853535566, 0.18177728497919932]': 115.44459839621052,\n",
       " '[0.2922873335864258, 0.5254980530126285, 0.1822146134009457]': 115.51204091912614,\n",
       " '[0.29810298738832564, 0.526005408378104, 0.17589160423357042]': 115.8522598577372,\n",
       " '[0.3151197458188876, 0.5092176904426292, 0.1756625637384832]': 116.36586133819418,\n",
       " '[0.34365095847730354, 0.4528562195263318, 0.20349282199636465]': 116.75554758204714,\n",
       " '[0.19802512531983996, 0.6738385738009652, 0.1281363008791948]': 116.9626821853069,\n",
       " '[0.2442761323458169, 0.31378473980872956, 0.4419391278454535]': 116.9738748753143,\n",
       " '[0.10134817329303802, 0.6424795049578578, 0.2561723217491042]': 117.43867058416863,\n",
       " '[0.2990286802363262, 0.5696904668290099, 0.13128085293466385]': 117.70905588895472,\n",
       " '[0.3079937124509837, 0.28888366727482895, 0.4031226202741873]': 117.986144989787,\n",
       " '[0.13397541167787863, 0.39383285305469906, 0.4721917352674223]': 118.0461241120872,\n",
       " '[0.24783354883045772, 0.6478149397662852, 0.10435151140325705]': 118.42326093594899,\n",
       " '[0.22153286604163125, 0.29401787488926256, 0.4844492590691062]': 118.49378342139626,\n",
       " '[0.36630718980906574, 0.46275544231842236, 0.1709373678725119]': 118.54090181059902,\n",
       " '[0.17708097229769515, 0.3182574532802064, 0.5046615744220985]': 118.87623234288799,\n",
       " '[0.2364331996186716, 0.66745875758808, 0.09610804279324836]': 118.94931084286814,\n",
       " '[0.2848820600485401, 0.6170695439534025, 0.09804839599805737]': 119.48149324427295,\n",
       " '[0.11351908627496277, 0.3999213691879, 0.48655954453713723]': 119.67781673299734,\n",
       " '[0.09860686220535353, 0.7273913319574079, 0.17400180583723857]': 119.69874630178278,\n",
       " '[0.34688821270095116, 0.2619854934267022, 0.39112629387234665]': 119.81790820430055,\n",
       " '[0.36694798940438056, 0.26843020781220195, 0.3646218027834175]': 119.97512038721123,\n",
       " '[0.3281300672566986, 0.25386523847970344, 0.418004694263598]': 119.98201017365449,\n",
       " '[0.26084781298018855, 0.6533686144405073, 0.08578357257930413]': 120.03966860986019,\n",
       " '[0.14006867260788014, 0.34019507156474815, 0.5197362558273717]': 120.09542223882283,\n",
       " '[0.3006178920721132, 0.24871384741135738, 0.4506682605165294]': 120.16296184057234,\n",
       " '[0.40235662667219196, 0.29419436598413073, 0.3034490073436773]': 120.36121045654305,\n",
       " '[0.2770870160510178, 0.639751333801874, 0.08316165014710819]': 120.55260384034368,\n",
       " '[0.14707838830393505, 0.31926837782115014, 0.5336532338749148]': 120.75636785706432,\n",
       " '[0.2140106600135775, 0.7128772049828284, 0.07311213500359415]': 120.96784748970121,\n",
       " '[0.24141262346219905, 0.23224737106738402, 0.5263400054704169]': 122.24381760157894,\n",
       " '[0.07568845505911825, 0.4709580432487446, 0.4533535016921372]': 122.34088723933074,\n",
       " '[0.07745284377702466, 0.4542205783663871, 0.46832657785658827]': 122.5671355756679,\n",
       " '[0.297476766120057, 0.21504025599184562, 0.48748297788809736]': 122.65338912616434,\n",
       " '[0.3276569460912001, 0.5968249358758975, 0.07551811803290243]': 122.70089855540601,\n",
       " '[0.2546746666901234, 0.688887188637518, 0.05643814467235864]': 123.04052186564222,\n",
       " '[0.39739023648667193, 0.21566218827266515, 0.3869475752406629]': 123.67476572136468,\n",
       " '[0.41332140792284033, 0.22336084975356163, 0.36331774232359804]': 123.72114924965616,\n",
       " '[0.357918886821883, 0.5705163639986457, 0.07156474917947131]': 124.33076859000838,\n",
       " '[0.47071643090765003, 0.3117769775627808, 0.2175065915295692]': 124.4922483440661,\n",
       " '[0.47226819456101155, 0.309331590314307, 0.21840021512468144]': 124.6233839118403,\n",
       " '[0.4716162987428326, 0.355988993007075, 0.1723947082500924]': 125.09116703448889,\n",
       " '[0.0855836248510885, 0.81443668980849, 0.09997968534042145]': 125.35930856180252,\n",
       " '[0.46393651768596755, 0.22303538373458942, 0.313028098579443]': 126.06294177660125,\n",
       " '[0.31202251077066945, 0.17735640233288863, 0.5106210868964419]': 126.0900216256548,\n",
       " '[0.38733174935070225, 0.17984239690568293, 0.4328258537436148]': 126.19784833426684,\n",
       " '[0.11827623106216589, 0.28010553083175893, 0.6016182381060752]': 126.4720688889531,\n",
       " '[0.39795506468316455, 0.533371468311237, 0.0686734670055984]': 126.66888935141674,\n",
       " '[0.15869861579047084, 0.22879348417709722, 0.6125079000324319]': 127.10915753213068,\n",
       " '[0.19264788730340687, 0.7774320591352111, 0.029920053561382054]': 127.17438278771698,\n",
       " '[0.48795928708886016, 0.22470611306009625, 0.2873345998510436]': 127.46981634071807,\n",
       " '[0.3731072191130438, 0.16257451167590886, 0.46431826921104735]': 127.69864885012855,\n",
       " '[0.3590149138915272, 0.5954807287685723, 0.04550435733990055]': 127.7761289291599,\n",
       " '[0.18590022886869517, 0.7867947699286181, 0.02730500120268675]': 127.81559014538787,\n",
       " '[0.059890957890395335, 0.818010067711511, 0.1220989743980937]': 128.42934511855105,\n",
       " '[0.49802093966542194, 0.36479900716892566, 0.1371800531656524]': 128.45607555870788,\n",
       " '[0.04369301642925305, 0.544309294214557, 0.41199768935618997]': 128.5552644335862,\n",
       " '[0.040416822003511044, 0.6812570586679835, 0.2783261193285055]': 128.9984513464541,\n",
       " '[0.4827810145531133, 0.18890945934537473, 0.328309526101512]': 129.01228818540713,\n",
       " '[0.11952088776376646, 0.8475612169263677, 0.032917895309865886]': 129.67243161860935,\n",
       " '[0.49416515690075413, 0.39676097722092574, 0.10907386587832013]': 129.71045613199016,\n",
       " '[0.5116229436765105, 0.18079422876352735, 0.30758282755996214]': 131.31019282525455,\n",
       " '[0.36517617820427994, 0.606619111462452, 0.0282047103332681]': 131.32298321426026,\n",
       " '[0.21511464920057455, 0.1573572201197473, 0.6275281306796782]': 131.91740266645346,\n",
       " '[0.13854254513579134, 0.20039885008618408, 0.6610586047780246]': 132.50605674157148,\n",
       " '[0.35017414933026336, 0.12313871344745064, 0.526687137222286]': 132.80994345531587,\n",
       " '[0.41740885772361735, 0.547421940721808, 0.03516920155457459]': 132.89004122679478,\n",
       " '[0.5505452047428953, 0.17387759294242044, 0.2755772023146843]': 134.67987549029715,\n",
       " '[0.30647263522237655, 0.6873308685141732, 0.006196496263450202]': 135.73804017220385,\n",
       " '[0.5846091843530776, 0.23826634579589123, 0.17712446985103114]': 136.55480451649083,\n",
       " '[0.27257162898030163, 0.11126875589342922, 0.6161596151262692]': 136.8794149021243,\n",
       " '[0.025422911779381052, 0.5126074186122986, 0.4619696696083203]': 138.5672233257315,\n",
       " '[0.5949960723119689, 0.18122966489280057, 0.2237742627952305]': 138.73665854508062,\n",
       " '[0.5511764561191675, 0.3826402754246917, 0.06618326845614075]': 139.21442138841678,\n",
       " '[0.4682545380979457, 0.5087560206622832, 0.02298944123977109]': 139.5737863878399,\n",
       " '[0.3371425894699146, 0.08472308896879943, 0.578134321561286]': 140.4515189083505,\n",
       " '[0.16790868521660884, 0.12595278367496787, 0.7061385311084233]': 141.74974669706685,\n",
       " '[0.4484292398248748, 0.07734215453762938, 0.4742286056374958]': 141.74996856982287,\n",
       " '[0.5399551267390775, 0.4178074294386743, 0.042237443822248144]': 141.86606317403815,\n",
       " '[0.6282222403604776, 0.2027192490168057, 0.1690585106227167]': 142.76526800286396,\n",
       " '[0.5625580468647223, 0.0948303435794896, 0.34261160955578807]': 143.33944819349082,\n",
       " '[0.3917369250707188, 0.06770198426402663, 0.5405610906652546]': 143.86054599268635,\n",
       " '[0.6272102238502685, 0.26844185780628094, 0.10434791834345059]': 144.75826320032812,\n",
       " '[0.4379795910441029, 0.06219555526587783, 0.4998248536900193]': 145.33700005698668,\n",
       " '[0.6465484276691827, 0.22885681885232634, 0.12459475347849092]': 146.509567776028,\n",
       " '[0.658423088911131, 0.15308934105066474, 0.18848757003820427]': 148.28354131086144,\n",
       " '[0.23592680814684874, 0.07539894621621057, 0.6886742456369407]': 148.3434510427464,\n",
       " '[0.06998107772791007, 0.17477561444671197, 0.755243307825378]': 150.07113593239737,\n",
       " '[0.013391295506466339, 0.4818344996040297, 0.504774204889504]': 152.19292350986638,\n",
       " '[0.6846521660357588, 0.213629642751189, 0.10171819121305214]': 154.23378250136116,\n",
       " '[0.499343026296187, 0.036472833273232586, 0.4641841404305804]': 154.57668468746058,\n",
       " '[0.5190911223451916, 0.037804393960181204, 0.4431044836946272]': 154.60177128344938,\n",
       " '[0.5638983448876447, 0.03582446840262665, 0.40027718670972867]': 157.18287401990176,\n",
       " '[0.023849532832432963, 0.9582929477765857, 0.017857519390981347]': 158.25353913626373,\n",
       " '[0.6883328948707207, 0.24643726153848622, 0.06522984359079309]': 158.7209200449769,\n",
       " '[0.3556327507182031, 0.02912410201166593, 0.615243147270131]': 159.70731013150925,\n",
       " '[0.5715210380553457, 0.4250668519470244, 0.003412109997629864]': 160.06704277685,\n",
       " '[0.3222240208034888, 0.031797691981438336, 0.6459782872150729]': 160.1101803877889,\n",
       " '[0.23564576205530563, 0.04443661583447067, 0.7199176221102237]': 160.97134250211096,\n",
       " '[0.0165386339722714, 0.30831331498950165, 0.675148051038227]': 161.22674500479275,\n",
       " '[0.5546488917353658, 0.02434389041106, 0.4210072178535742]': 161.59165168954556,\n",
       " '[0.7150443288496778, 0.21023400837806427, 0.07472166277225789]': 162.6778121800764,\n",
       " '[0.036186409317652934, 0.18706431199409046, 0.7767492786882566]': 162.76185568353742,\n",
       " '[0.009861967649685166, 0.9089140536452274, 0.08122397870508746]': 165.052495651554,\n",
       " '[0.6294001322338357, 0.36221039034135516, 0.00838947742480911]': 165.15908598920072,\n",
       " '[0.7419757810084356, 0.1270929868992533, 0.13093123209231106]': 165.6313110227884,\n",
       " '[0.05219036117139675, 0.13947895469421545, 0.8083306841343878]': 165.63223924277574,\n",
       " '[0.6933344741600952, 0.034963574907231565, 0.27170195093267324]': 168.94006874386844,\n",
       " '[0.7110370818618461, 0.04237448600633209, 0.2465884321318218]': 169.2956055300282,\n",
       " '[0.7553644732292927, 0.137841487287509, 0.10679403948319832]': 169.52707236070663,\n",
       " '[0.741641917846974, 0.22940414479498128, 0.028953937358044746]': 178.3551456151966,\n",
       " '[0.17591773709639758, 0.03126990031767962, 0.7928123625859228]': 179.17148294064367,\n",
       " '[0.7852132360222064, 0.06053711900433345, 0.15424964497346016]': 180.86670272863,\n",
       " '[0.80489262154294, 0.0806493489127984, 0.11445802954426165]': 185.22247897267238,\n",
       " '[0.7307471489659672, 0.008561593741951867, 0.260691257292081]': 187.12404264351915,\n",
       " '[0.7388094214037092, 0.0038636176057067484, 0.25732696099058405]': 192.87615040391464,\n",
       " '[0.8395865452491138, 0.03685298237743395, 0.12356047237345225]': 202.48317555812716,\n",
       " '[0.19808887254428675, 0.007497716428619738, 0.7944134110270935]': 203.32689472089325,\n",
       " '[0.8368817998743093, 0.026723035165893916, 0.13639516495979676]': 203.9802455509686,\n",
       " '[0.8410517507160463, 0.03234622648171037, 0.1266020228022433]': 204.04410590471332,\n",
       " '[0.8628268057586561, 0.09005398060359637, 0.047119213637747515]': 214.9009568974202,\n",
       " '[0.0012986301607608164, 0.3646606668262935, 0.6340407030129457]': 220.71036213759754,\n",
       " '[0.04657142193596975, 0.014014590087778944, 0.9394139879762513]': 282.5517832353307}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_avgperlexity = dict(sorted(lambda_avgperlexity.items(), key=operator.itemgetter(1),reverse=False))\n",
    "lambda_avgperlexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report each perplexity for each test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files= os.listdir('test') #get all file names under the training dataset\n",
    "test_articles = []\n",
    "for file in text_files: \n",
    "    f = open('test/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    test_articles.append(article.lower())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_test_articles = []\n",
    "for i in test_articles:\n",
    "    test_sentences = token_sentence(i)\n",
    "    tokened_test_sentences = []\n",
    "    for j in test_sentences:\n",
    "        temp_test_sentence = token(j)\n",
    "        tokened_test_sentences.append(temp_test_sentence)\n",
    "    tokened_test_articles.append(tokened_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first test file has perplexity 534.8147659788204, while the second test file has perplexity 114.91933361716188\n"
     ]
    }
   ],
   "source": [
    "print('The first test file has perplexity ' + str(get_perlexities(tokened_test_articles[0], training_unigram_prob, training_bigram_prob, \n",
    "                training_trigram_prob,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145]))+\n",
    "      ', while the second test file has perplexity '\n",
    "      + str(get_perlexities(tokened_test_articles[1], training_unigram_prob, training_bigram_prob, \n",
    "                training_trigram_prob,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build another language model with add-λ smoothing. Use λ = 0.1 and λ = 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probability_smoothing(unigram_dic, length, smoothing, V):\n",
    "    unigram_prob_dic = {}\n",
    "    for j in unigram_dic.keys():\n",
    "        unigram_prob_dic[j] = (unigram_dic[j]+smoothing)/(length+V * smoothing)\n",
    "    return unigram_prob_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probability_smoothing(unigram_dic, bigram_dic, smoothing, V):\n",
    "    bigram_prob_dic = {}\n",
    "    for j in bigram_dic.keys():\n",
    "        bigram_words = j.split('|') #in j, after split, bigram_words[1] is Wi-1, bigram_words[0] is Wi,\n",
    "        bigram_prob_dic[j] = (bigram_dic[j]+smoothing)/(unigram_dic[bigram_words[1]]+V*smoothing)\n",
    "    return bigram_prob_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_probability_smoothing(bigram_dic, trigram_dic, smoothing, V):\n",
    "    trigram_prob_dic = {}\n",
    "    for j in trigram_dic.keys():\n",
    "        trigram_words = j.split('|',1) #in j, after split, trigram_words[1] is Wi-2||Wi-1, bigram_words[0] is Wi\n",
    "        trigram_prob_dic[j] = (trigram_dic[j]+smoothing)/(bigram_dic[trigram_words[1].split('||')[1] + '|' \n",
    "                                                    + trigram_words[1].split('||')[0]] + V*smoothing)\n",
    "                                        #transfer Wi-2||Wi-1 to Wi|Wi-1 to match the bigram_dic count format\n",
    "        #Probability of trigram = Count(Wi-2, Wi-1, Wi)/Count(Wi-2, Wi-1)\n",
    "        #Count(Wi-2, Wi-1) = bigram_dic[trigram_words[1].split('||')[1] + '|' + trigram_words[1].split('||')[0]]\n",
    "    return trigram_prob_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perlexities_smoothing(text, unigram_dic, bigram_dic, trigram_dic, \n",
    "                              unigram_prob, bigram_prob, trigram_prob, lambdas, smoothing, V):\n",
    "    #lambda_avgperlexity = {}\n",
    "    #for lam in lambdas:\n",
    "    lam1 = lambdas[0]\n",
    "    lam2 = lambdas[1]\n",
    "    lam3 = lambdas[2]\n",
    "    P_sentence = 0\n",
    "    word_len = 0\n",
    "    for sentence in text:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "                \n",
    "            unigram_format = sentence[i]\n",
    "            if sentence[i] in unigram_prob:\n",
    "                unigram_format= sentence[i]\n",
    "                \n",
    "            else:\n",
    "                unigram_format = 'UNK'\n",
    "                \n",
    "            p_uni = unigram_prob[unigram_format]\n",
    "            \n",
    "            p_bi = smoothing / (smoothing* V)\n",
    "            p_tri = smoothing / (smoothing* V)\n",
    "            \n",
    "            # When calculating the bigram probability with bigram not in training data, consider two conditions: \n",
    "            # sentence[i-1] is in the training dataset or not\n",
    "            if i == 1:\n",
    "                if sentence[i-1] in unigram_prob:\n",
    "                    bi_i_1 = sentence[i-1]\n",
    "                else:\n",
    "                    bi_i_1 = 'UNK'\n",
    "                bigram_format = unigram_format+'|'+ bi_i_1\n",
    "                if bigram_format in bigram_prob:\n",
    "                    p_bi = bigram_prob[bigram_format]\n",
    "                else:\n",
    "                    if bi_i_1 in unigram_dic:\n",
    "                        p_bi = smoothing / (unigram_dic[bi_i_1] + smoothing* V)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            if i != 0 and i !=1:\n",
    "                if sentence[i-1] in unigram_prob:\n",
    "                    bi_i_1 = sentence[i-1]\n",
    "                else:\n",
    "                    bi_i_1 = 'UNK'\n",
    "                \n",
    "                if sentence[i-2] in unigram_prob:\n",
    "                    tri_i_2 = sentence[i-2]\n",
    "                else:\n",
    "                    tri_i_2 = 'UNK'\n",
    "                \n",
    "                bigram_format = unigram_format+'|'+ bi_i_1\n",
    "                trigram_format = unigram_format+'|'+tri_i_2+'||' + bi_i_1\n",
    "                    \n",
    "                if bigram_format in bigram_prob:\n",
    "                    p_bi = bigram_prob[bigram_format]\n",
    "                if bigram_format not in bigram_prob:\n",
    "                    p_bi = smoothing / (unigram_dic[bi_i_1] + smoothing* V)\n",
    "                    \n",
    "                    \n",
    "                # When calculating the trigram probability with trigram not in training data, consider two conditions: \n",
    "                # sentence[i-1]|sentence[i-2] is in the training dataset or not    \n",
    "                if trigram_format in trigram_prob:\n",
    "                    p_tri = trigram_prob[trigram_format]\n",
    "                if trigram_format not in trigram_prob:\n",
    "                    if bi_i_1+\"|\"+tri_i_2 in bigram_dic:\n",
    "                        p_tri = smoothing / (bigram_dic[bi_i_1+\"|\"+tri_i_2] + smoothing* V)\n",
    "                    \n",
    "                    \n",
    "            p_word = lam1*p_uni + lam2*p_bi + lam3*p_tri\n",
    "            \n",
    "            P_sentence = P_sentence + np.log(p_word)  #add p_words_in_a_sentence on the existing p_sentence\n",
    "        word_len = word_len + len(sentence)\n",
    "        \n",
    "    perplexity = pow((np.e),(-1/word_len)*P_sentence)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_training_unigram_prob_1 = unigram_probability_smoothing(training_unigram_dic, training_token_length, 0.1, len(Unigram_counts))\n",
    "smoothing_training_bigram_prob_1 = bigram_probability_smoothing(training_unigram_dic, training_bigram_dic,0.1,len(Unigram_counts))\n",
    "smoothing_training_trigram_prob_1 = trigram_probability_smoothing(training_bigram_dic, training_trigram_dic,0.1,len(Unigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When λ=0.1, The first test file has perplexity 549.0875200787602, while the second test file has perplexity 219.55791746228195\n"
     ]
    }
   ],
   "source": [
    "print('When λ=0.1, The first test file has perplexity ' + str(get_perlexities_smoothing(tokened_test_articles[0], \n",
    "        training_unigram_dic, training_bigram_dic, training_trigram_dic, smoothing_training_unigram_prob_1, smoothing_training_bigram_prob_1, \n",
    "                smoothing_training_trigram_prob_1,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145], 0.1, len(Unigram_counts)))\n",
    "      + ', while the second test file has perplexity '\n",
    "      + str(get_perlexities_smoothing(tokened_test_articles[1], \n",
    "        training_unigram_dic, training_bigram_dic, training_trigram_dic, smoothing_training_unigram_prob_1, smoothing_training_bigram_prob_1, \n",
    "                smoothing_training_trigram_prob_1,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145], 0.1, len(Unigram_counts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_training_unigram_prob_3 = unigram_probability_smoothing(training_unigram_dic, training_token_length, 0.3, len(Unigram_counts))\n",
    "smoothing_training_bigram_prob_3 = bigram_probability_smoothing(training_unigram_dic, training_bigram_dic,0.3,len(Unigram_counts))\n",
    "smoothing_training_trigram_prob_3 = trigram_probability_smoothing(training_bigram_dic, training_trigram_dic,0.3,len(Unigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When λ=0.3, The first test file has perplexity 629.470804626108, while the second test file has perplexity 295.00018220011725\n"
     ]
    }
   ],
   "source": [
    "print('When λ=0.3, The first test file has perplexity ' + str(get_perlexities_smoothing(tokened_test_articles[0], \n",
    "        training_unigram_dic, training_bigram_dic, training_trigram_dic, smoothing_training_unigram_prob_3, smoothing_training_bigram_prob_3, \n",
    "                smoothing_training_trigram_prob_3,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145], 0.3, len(Unigram_counts)))\n",
    "      + ', while the second test file has perplexity '\n",
    "      + str(get_perlexities_smoothing(tokened_test_articles[1], \n",
    "        training_unigram_dic, training_bigram_dic, training_trigram_dic, smoothing_training_unigram_prob_3, smoothing_training_bigram_prob_3, \n",
    "                smoothing_training_trigram_prob_3,[0.16461176032353309, 0.5598203068649524, 0.2755679328115145], 0.3, len(Unigram_counts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
