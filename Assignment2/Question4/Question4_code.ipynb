{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize, sent_tokenize\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.pipeline import Sentencizer\n",
    "import operator\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "import readability\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.preprocessing import Normalizer,power_transform\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_quality/train_data.json\",'r') as fin:\n",
    "    train_data = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summary_quality/test_data.json\",'r') as fin:\n",
    "    test_data = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files= os.listdir('summary_quality/summaries') #get all file names under the training dataset\n",
    "\n",
    "for file in list(train_data): \n",
    "    f = open('summary_quality/summaries/'+file, mode = 'r',encoding='latin-1')\n",
    "    article = f.read()\n",
    "    #all_text.append(article.lower()) # lowercase all the words\n",
    "    f.close()\n",
    "    Summary_list.append([file, article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_summary = []\n",
    "for file in list(test_data): \n",
    "    f = open('summary_quality/summaries/'+file, mode = 'r',encoding='latin-1')\n",
    "    article = f.read()\n",
    "    #all_text.append(article.lower()) # lowercase all the words\n",
    "    f.close()\n",
    "    test_summary.append([file, article])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_it(text):\n",
    "    doc = nlp(text)\n",
    "    unigram = []\n",
    "    bigram = []\n",
    "    for i in range(len(doc)):\n",
    "        unigram.append(doc[i].text)\n",
    "        if i>=1:\n",
    "            bigram.append([doc[i-1].text, doc[i].text])\n",
    "    return unigram, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_token(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_repetitive(gram_List):\n",
    "    repetitive_num = 0\n",
    "    for j in range(len(gram_List)):\n",
    "        if j >= 1:\n",
    "            if str(gram_List[j]) == str(gram_List[j-1]):\n",
    "                #print(str(gram_List[j]),str(gram_List[j-1]))\n",
    "                repetitive_num +=1\n",
    "            \n",
    "    return repetitive_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_score(all_sentences):\n",
    "    read_score = []\n",
    "    for i in all_sentences:\n",
    "\n",
    "        if any(c.isalpha() for c in str(i)) is True:\n",
    "            \n",
    "            text = (str(i))\n",
    "        \n",
    "            results = readability.getmeasures(text, lang='en')\n",
    "            read_score.append(results['readability grades']['FleschReadingEase'])\n",
    "    if len(read_score)>0:\n",
    "        return read_score\n",
    "    else:\n",
    "        return [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_vector(data_dic, filename):\n",
    "    temp_dic = data_dic[filename]\n",
    "    sent_vec = [int(temp_dic['coherence']),int(temp_dic['grammaticality']),int(temp_dic['nonredundancy'])]\n",
    "    \n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_Score_Vec = []\n",
    "Grammaticality_Score_Y = []\n",
    "for i in Summary_list:\n",
    "    unigram, bigrm = token_it(i[1].lower().replace('\\n',' '))\n",
    "    \n",
    "    sentences = sentence_token(i[1])\n",
    "    temp_vector = [count_repetitive(unigram), count_repetitive(list(bigrm)),\n",
    "                   min(readability_score(sentences))]\n",
    "    Grammaticality_Score_Y.append(summary_vector(train_data, i[0])[1])\n",
    "    Grammaticality_Score_Vec.append(temp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_test_Vec = []\n",
    "Grammaticality_test_Y = []\n",
    "for i in test_summary:\n",
    "\n",
    "    unigram, bigrm = token_it(i[1].lower().replace('\\n',' '))\n",
    "    \n",
    "    sentences = sentence_token(i[1])\n",
    "    temp_vector = [count_repetitive(unigram), count_repetitive(list(bigrm)),\n",
    "                   min(readability_score(sentences))]\n",
    "    Grammaticality_test_Y.append(summary_vector(test_data, i[0])[1])\n",
    "    Grammaticality_test_Vec.append(temp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xizheng/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gramma = svm.SVC(gamma='auto_deprecated')\n",
    "clf_gramma.fit(Grammaticality_Score_Vec, Grammaticality_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gramma_1 = clf_gramma.predict(Grammaticality_test_Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_MSE_1 = mean_squared_error(Grammaticality_test_Y,predict_gramma_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_pearsonr_1 = pearsonr(Grammaticality_test_Y,predict_gramma_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.7564766839378239. The pearson is (0.20775594900478267, 0.0037412836241748322).\n"
     ]
    }
   ],
   "source": [
    "print('The MSE is '+str(gramma_MSE_1) +'. The pearson is '+str(gramma_pearsonr_1) +'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_repeat_uni(texts):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(texts)\n",
    "    uni = {}\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text in uni:\n",
    "            uni[doc[i].text] += 1\n",
    "        if doc[i].text not in uni:\n",
    "            uni[doc[i].text] = 1\n",
    "    \n",
    "    repeat_grams = 0\n",
    "    for i in list(uni.keys()):\n",
    "        if uni[i] >1:\n",
    "            repeat_grams +=1\n",
    "    return repeat_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countgram(gram_List):\n",
    "    count_dic = {}\n",
    "    \n",
    "    for j in gram_List:\n",
    "        if str(j) in count_dic:\n",
    "            count_dic[str(j)] += 1\n",
    "        if str(j) not in count_dic:\n",
    "            count_dic[str(j)] = 1\n",
    "    return count_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_Score_Vec_1 = []\n",
    "\n",
    "for i in range(len(Grammaticality_Score_Vec)):\n",
    "    temp_text = Summary_list[i][1]\n",
    "    unigram, bigrm = token_it(temp_text.lower().replace('\\n',' '))\n",
    "    Grammaticality_Score_Vec_1.append(Grammaticality_Score_Vec[i]+\n",
    "                                      [len(countgram(unigram))/len(sentence_token(temp_text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_test_Vec_1 = []\n",
    "\n",
    "for i in range(len(Grammaticality_test_Vec)):\n",
    "    temp_text = test_summary[i][1]\n",
    "    unigram, bigrm = token_it(temp_text.lower().replace('\\n',' '))\n",
    "    Grammaticality_test_Vec_1.append(Grammaticality_test_Vec[i]+\n",
    "                                      [len(countgram(unigram))/len(sentence_token(temp_text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    return power_transform(X, method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_Score_Vec_1_norm = normalization(Grammaticality_Score_Vec_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_test_Vec_1_norm = normalization(Grammaticality_test_Vec_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gramma = svm.SVC(gamma='auto_deprecated')\n",
    "clf_gramma.fit(Grammaticality_Score_Vec_1_norm, Grammaticality_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gramma_2 = clf_gramma.predict(Grammaticality_test_Vec_1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_MSE_2 = mean_squared_error(Grammaticality_test_Y,predict_gramma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_pearsonr_2 = pearsonr(Grammaticality_test_Y,predict_gramma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.7150259067357513. The pearson is (0.2821574928897053, 7.023937045149804e-05).\n"
     ]
    }
   ],
   "source": [
    "print('The MSE is '+str(gramma_MSE_2) +'. The pearson is '+str(gramma_pearsonr_2) +'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_Score_Vec_2 = []\n",
    "\n",
    "for i in range(len(Grammaticality_Score_Vec)):\n",
    "    temp_text = Summary_list[i][1]\n",
    "    unigram, bigrm = token_it(temp_text.lower().replace('\\n',' '))\n",
    "    Grammaticality_Score_Vec_2.append(Grammaticality_Score_Vec[i]+\n",
    "                                      [Grammaticality_Score_Vec[i][0]/len(unigram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grammaticality_test_Vec_2 = []\n",
    "\n",
    "for i in range(len(Grammaticality_test_Vec)):\n",
    "    temp_text = test_summary[i][1]\n",
    "    Grammaticality_test_Vec_2.append(Grammaticality_test_Vec[i]+\n",
    "                                      [Grammaticality_test_Vec[i][0]/len(unigram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xizheng/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gramma = svm.SVC(gamma='auto_deprecated')\n",
    "clf_gramma.fit(Grammaticality_Score_Vec_2, Grammaticality_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gramma_3 = clf_gramma.predict(Grammaticality_test_Vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_MSE_3 = mean_squared_error(Grammaticality_test_Y,predict_gramma_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramma_pearsonr_3 = pearsonr(Grammaticality_test_Y,predict_gramma_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.7564766839378239. The pearson is (0.20775594900478267, 0.0037412836241748322).\n"
     ]
    }
   ],
   "source": [
    "print('The MSE is '+str(gramma_MSE_3) +'. The pearson is '+str(gramma_pearsonr_3) +'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vecs = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(grams):\n",
    "    new_grams = []\n",
    "    for i in grams:\n",
    "        if str(i) not in spacy_stopwords:\n",
    "            new_grams.append(i)\n",
    "    return new_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sent_list):\n",
    "    sentence_embedding = []\n",
    "    for i in sent_list:\n",
    "        words_embedding = []\n",
    "        for words in str(i).split(' '):\n",
    "            if str(words).lower() in google_vecs:\n",
    "                words_embedding.append(google_vecs[str(words).lower()])\n",
    "            else:\n",
    "                words_embedding.append(np.zeros(google_vecs.vector_size))\n",
    "        sentence_embedding.append(np.mean(words_embedding,axis=0))\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity(embedding):\n",
    "    cosSim = []\n",
    "    for i in embedding:\n",
    "        for j in embedding:\n",
    "            if str(i) !=str(j):\n",
    "                cosSim.append(cosine_similarity([i], [j])[0][0])\n",
    "    if len(cosSim) == 0:\n",
    "        max_sim = 0\n",
    "    else:\n",
    "        max_sim = sorted(cosSim, reverse=True)[0]\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec = []\n",
    "Redundancy_Score_Y = []\n",
    "for i in Summary_list:\n",
    "    unigram, bigrm = token_it(i[1].lower().replace('\\n',' '))\n",
    "    unigram_dic = dict(sorted(countgram(remove_stop(unigram)).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    bigram_dic = dict(sorted(countgram(bigrm).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sentences = sentence_token(i[1])\n",
    "    temp_vector = [unigram_dic[list(unigram_dic)[0]], bigram_dic[list(bigram_dic)[0]],generate_similarity(compute_similarity(sentences))]\n",
    "\n",
    "    Redundancy_Score_Vec.append(temp_vector)\n",
    "    Redundancy_Score_Y.append(summary_vector(train_data, i[0])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec_norm = normalization(Redundancy_Score_Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec = []\n",
    "Redundancy_test_Y = []\n",
    "for i in test_summary:\n",
    "    unigram, bigrm = token_it(i[1].lower().replace('\\n',' '))\n",
    "    unigram_dic = dict(sorted(countgram(remove_stop(unigram)).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    bigram_dic = dict(sorted(countgram(bigrm).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sentences = sentence_token(i[1])\n",
    "    temp_vector = [unigram_dic[list(unigram_dic)[0]], bigram_dic[list(bigram_dic)[0]],generate_similarity(compute_similarity(sentences))]\n",
    "\n",
    "    Redundancy_test_Vec.append(temp_vector)\n",
    "    Redundancy_test_Y.append(summary_vector(test_data, i[0])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec_norm = normalization(Redundancy_test_Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_red = svm.SVC(gamma='auto_deprecated')\n",
    "clf_red.fit(Redundancy_Score_Vec_norm, Redundancy_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.8031088082901554. The pearson is (0.11481526234107749, 0.111842589457841).\n"
     ]
    }
   ],
   "source": [
    "predict_red_1 = clf_red.predict(Redundancy_test_Vec_norm)\n",
    "red_MSE_1 = mean_squared_error(Redundancy_test_Y,predict_red_1)\n",
    "red_pearsonr_1 = pearsonr(Redundancy_test_Y,predict_red_1)\n",
    "print('The MSE is '+str(red_MSE_1) +'. The pearson is '+str(red_pearsonr_1) +'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find each token's pos, calculate the frequencies of all the pos associate with the tokens, and use the maximum value as a new feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos(text):\n",
    "    pos_dic = {}\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for token in nlp(text):\n",
    "        ps = token.pos_\n",
    "        if str(ps) in pos_dic:\n",
    "            pos_dic[str(ps)] +=1\n",
    "        if str(ps) not in pos_dic:\n",
    "            pos_dic[str(ps)] =1\n",
    "    return pos_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec_1 = []\n",
    "\n",
    "for i in range(len(Redundancy_Score_Vec)):\n",
    "    temp_text = Summary_list[i][1]\n",
    "    unigram_pos = dict(sorted(find_pos(temp_text).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    Redundancy_Score_Vec_1.append(Redundancy_Score_Vec[i]+\n",
    "                                      [unigram_pos[list(unigram_pos)[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec_1 = []\n",
    "\n",
    "for i in range(len(Redundancy_test_Vec)):\n",
    "    temp_text = test_summary[i][1]\n",
    "    unigram_pos = dict(sorted(find_pos(temp_text).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    Redundancy_test_Vec_1.append(Redundancy_test_Vec[i]+\n",
    "                                      [unigram_pos[list(unigram_pos)[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec_1_norm = normalization(Redundancy_Score_Vec_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec_1_norm = normalization(Redundancy_test_Vec_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_red = svm.SVC(gamma='auto_deprecated')\n",
    "clf_red.fit(Redundancy_Score_Vec_1_norm, Redundancy_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.7979274611398963. The pearson is (0.13338004311664917, 0.06442765910550147).\n"
     ]
    }
   ],
   "source": [
    "predict_red_2 = clf_red.predict(Redundancy_test_Vec_1_norm)\n",
    "red_MSE_2 = mean_squared_error(Redundancy_test_Y,predict_red_2)\n",
    "red_pearsonr_2 = pearsonr(Redundancy_test_Y,predict_red_2)\n",
    "print('The MSE is '+str(red_MSE_2) +'. The pearson is '+str(red_pearsonr_2) +'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find each token's tag, calculate the frequencies of all the tag associate with the tokens, and use the maximum value as a new feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(text):\n",
    "    tg_dic = {}\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for token in nlp(text):\n",
    "        tg = token.tag_\n",
    "        if str(tg) in tg_dic:\n",
    "            tg_dic[str(tg)] +=1\n",
    "        if str(tg) not in tg_dic:\n",
    "            tg_dic[str(tg)] =1\n",
    "    return tg_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec_2 = []\n",
    "\n",
    "for i in range(len(Redundancy_Score_Vec)):\n",
    "    temp_text = Summary_list[i][1]\n",
    "    unigram_tag= dict(sorted(find_tag(temp_text).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    Redundancy_Score_Vec_2.append(Redundancy_Score_Vec[i]+\n",
    "                                      [unigram_tag[list(unigram_tag)[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec_2 = []\n",
    "\n",
    "for i in range(len(Redundancy_test_Vec)):\n",
    "    temp_text = test_summary[i][1]\n",
    "    unigram_tag = dict(sorted(find_tag(temp_text).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    Redundancy_test_Vec_2.append(Redundancy_test_Vec[i]+\n",
    "                                      [unigram_tag[list(unigram_tag)[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_Score_Vec_2_norm = normalization(Redundancy_Score_Vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "Redundancy_test_Vec_2_norm = normalization(Redundancy_test_Vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_red = svm.SVC(gamma='auto_deprecated')\n",
    "clf_red.fit(Redundancy_Score_Vec_2_norm, Redundancy_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 0.8031088082901554. The pearson is (0.12965330171063671, 0.07232078748755091).\n"
     ]
    }
   ],
   "source": [
    "predict_red_3 = clf_red.predict(Redundancy_test_Vec_2_norm)\n",
    "red_MSE_3 = mean_squared_error(Redundancy_test_Y,predict_red_3)\n",
    "red_pearsonr_3 = pearsonr(Redundancy_test_Y,predict_red_3)\n",
    "print('The MSE is '+str(red_MSE_3) +'. The pearson is '+str(red_pearsonr_3) +'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everytime I use doc._.coref_clusters function, I got a deadkernel and can not use the function. Hence I'll build the noun chunks vector first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_chunks(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    chunk_dic = {}\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if str(chunk) in chunk_dic:\n",
    "            chunk_dic[str(chunk)] +=1\n",
    "        if str(chunk) not in chunk_dic:\n",
    "            chunk_dic[str(chunk)] =1\n",
    "    repeated_chunks = 0\n",
    "    for i in chunk_dic: \n",
    "        if chunk_dic[i]>1:\n",
    "            repeated_chunks+=1\n",
    "            \n",
    "    return repeated_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence_Score_Vec = []\n",
    "Coherence_Score_Y = []\n",
    "for i in Summary_list:\n",
    "    temp_text = i[1].lower().replace('\\n',' ')\n",
    "    temp_vec = [count_noun_chunks(temp_text)]\n",
    "    Coherence_Score_Vec.append(temp_vec)\n",
    "    Coherence_Score_Y.append(summary_vector(train_data, i[0])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence_Score_Vec_norm = normalization(Coherence_Score_Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence_test_Vec = []\n",
    "Coherence_test_Y = []\n",
    "for i in test_summary:\n",
    "    temp_text = i[1].lower().replace('\\n',' ')\n",
    "    temp_vec = [count_noun_chunks(temp_text)]\n",
    "    Coherence_test_Vec.append(temp_vec)\n",
    "    Coherence_test_Y.append(summary_vector(test_data, i[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence_test_Vec_norm = normalization(Coherence_test_Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_coh = svm.SVC(gamma='auto_deprecated')\n",
    "clf_coh.fit(Coherence_Score_Vec_norm, Coherence_Score_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_coh = clf_coh.predict(Coherence_test_Vec_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_MSE_1 = mean_squared_error(Coherence_test_Y,predict_coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xizheng/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:3038: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "coh_pearsonr_1 = pearsonr(Coherence_test_Y,predict_coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is 1.2124352331606219. The pearson is (nan, 1.0).\n"
     ]
    }
   ],
   "source": [
    "print('The MSE is '+str(coh_MSE_1) +'. The pearson is '+str(coh_pearsonr_1) +'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xizheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/xizheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/xizheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model\n",
    "import neuralcoref\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('My sister has a dog. She loves him.')\n",
    "\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
