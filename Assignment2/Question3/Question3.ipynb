{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize, sent_tokenize\n",
    "import os\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xizheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/neg') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/neg/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    #all_text.append(article.lower()) # lowercase all the words\n",
    "    all_text = all_text + article.lower()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/pos') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/pos/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    #all_text.append(article.lower()) # lowercase all the words\n",
    "    all_text = all_text + article.lower()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenit(text):\n",
    "    text_token = wordpunct_tokenize(text)\n",
    "    return text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allwords = tokenit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Allwords_dic = {}\n",
    "for i in Allwords:\n",
    "    if i in Allwords_dic:\n",
    "        Allwords_dic[i] += 1\n",
    "    if i not in Allwords_dic:\n",
    "        Allwords_dic[i] = 1\n",
    "Allwords_dic = dict(sorted(Allwords_dic.items(), key=operator.itemgetter(1),reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use stop words to get rid of some meaningless words\n",
    "stopwords = set(stopwords.words('english'))\n",
    "VOC_withstopwords = list(Allwords_dic.keys())[:600]\n",
    "VOC = []\n",
    "for i in VOC_withstopwords:\n",
    "    if i not in stopwords:\n",
    "        VOC.append(i)  #this is the vocabulary used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_feature = [] #this is X\n",
    "Sentiment = [] #this is y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/neg') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/neg/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    temp_feature = []\n",
    "    for i in VOC:\n",
    "        if i in windows:\n",
    "            temp_feature.append(1)\n",
    "        else:\n",
    "            temp_feature.append(0)\n",
    "    words_count_feature.append(temp_feature)\n",
    "    Sentiment.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/pos') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/pos/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    temp_feature = []\n",
    "    for i in VOC:\n",
    "        if i in windows:\n",
    "            temp_feature.append(1)\n",
    "        else:\n",
    "            temp_feature.append(0)\n",
    "    words_count_feature.append(temp_feature)\n",
    "    Sentiment.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_fold_multilayer(X, y, hidden_layer,activation_func):\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    ini = 0\n",
    "    accuracy = []\n",
    "    for i in range(10):\n",
    "        \n",
    "        TrainingX = X[0:ini]+X[ini+999:len(X)]\n",
    "        Trainingy = y[0:ini]+y[ini+999:len(X)]\n",
    "        testX = X[ini:ini+999]\n",
    "        testy = y[ini:ini+999]\n",
    "        clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=hidden_layer, activation = activation_func,random_state = 1)\n",
    "        clf.fit(TrainingX, Trainingy)\n",
    "        y_pred = clf.predict(testX)\n",
    "        accuracy.append(accuracy_score(testy, y_pred))\n",
    "    print('With hidden layer ' +str(hidden_layer)+', using activation function '+activation_func\n",
    "          +', the average accuracy to predict sentiment score is '+ str(sum(accuracy)/len(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hidden_Layers = [(50,10),(250,10),(600,10),(1000,10)]\n",
    "Activation_Func = ['relu','identity','tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With hidden layer (50, 10), using activation function relu, the average accuracy to predict sentiment score is 0.912912912912913\n",
      "With hidden layer (50, 10), using activation function identity, the average accuracy to predict sentiment score is 0.914914914914915\n",
      "With hidden layer (50, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.907907907907908\n",
      "With hidden layer (250, 10), using activation function relu, the average accuracy to predict sentiment score is 0.922922922922923\n",
      "With hidden layer (250, 10), using activation function identity, the average accuracy to predict sentiment score is 0.914914914914915\n",
      "With hidden layer (250, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9119119119119119\n",
      "With hidden layer (600, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9219219219219219\n",
      "With hidden layer (600, 10), using activation function identity, the average accuracy to predict sentiment score is 0.914914914914915\n",
      "With hidden layer (600, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9069069069069069\n",
      "With hidden layer (1000, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9169169169169169\n",
      "With hidden layer (1000, 10), using activation function identity, the average accuracy to predict sentiment score is 0.914914914914915\n",
      "With hidden layer (1000, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9139139139139141\n"
     ]
    }
   ],
   "source": [
    "for i in Hidden_Layers:\n",
    "    for j in Activation_Func:\n",
    "        ten_fold_multilayer(words_count_feature, Sentiment, i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3_2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(250,10), activation = 'relu',random_state = 1)\n",
    "clf3_2.fit(words_count_feature, Sentiment)\n",
    "y_pred_3_2 = clf3_2.predict(words_count_feature)\n",
    "accuracy_3_2 = accuracy_score(Sentiment, y_pred_3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_3_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vecs = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avgwordembeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/neg') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/neg/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    temp_feature = []\n",
    "    for i in windows:\n",
    "        if i in google_vecs.vocab:\n",
    "            temp_feature.append(google_vecs[i])\n",
    "        else:\n",
    "            temp_feature.append(np.zeros(google_vecs.vector_size))\n",
    "    Avgwordembeddings.append(np.mean(temp_feature,axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/pos') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/pos/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    temp_feature = []\n",
    "    for i in windows:\n",
    "        if i in google_vecs.vocab:\n",
    "            temp_feature.append(google_vecs[i])\n",
    "        else:\n",
    "            temp_feature.append(np.zeros(google_vecs.vector_size))\n",
    "    Avgwordembeddings.append(np.mean(temp_feature,axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With hidden layer (50, 10), using activation function relu, the average accuracy to predict sentiment score is 0.922922922922923\n",
      "With hidden layer (50, 10), using activation function identity, the average accuracy to predict sentiment score is 0.9189189189189191\n",
      "With hidden layer (50, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.922922922922923\n",
      "With hidden layer (250, 10), using activation function relu, the average accuracy to predict sentiment score is 0.922922922922923\n",
      "With hidden layer (250, 10), using activation function identity, the average accuracy to predict sentiment score is 0.9169169169169169\n",
      "With hidden layer (250, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9239239239239241\n",
      "With hidden layer (600, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9219219219219219\n",
      "With hidden layer (600, 10), using activation function identity, the average accuracy to predict sentiment score is 0.917917917917918\n",
      "With hidden layer (600, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9159159159159158\n",
      "With hidden layer (1000, 10), using activation function relu, the average accuracy to predict sentiment score is 0.927927927927928\n",
      "With hidden layer (1000, 10), using activation function identity, the average accuracy to predict sentiment score is 0.9189189189189191\n",
      "With hidden layer (1000, 10), using activation function tanh, the average accuracy to predict sentiment score is 0.9189189189189191\n"
     ]
    }
   ],
   "source": [
    "# We can still use the Sentiment list as y since we follow the same order as one hot encoding to build average\n",
    "#word embeddings' X\n",
    "for i in Hidden_Layers:\n",
    "    for j in Activation_Func:\n",
    "        ten_fold_multilayer(Avgwordembeddings, Sentiment, i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3_3 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000,10), activation = 'relu',random_state = 1)\n",
    "clf3_3.fit(Avgwordembeddings, Sentiment)\n",
    "y_pred_3_3 = clf3_3.predict(Avgwordembeddings)\n",
    "accuracy_3_3 = accuracy_score(Sentiment, y_pred_3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9455"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_3_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/neg') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/neg/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    summary_list.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "files= os.listdir('training/pos') #get all file names under the training dataset\n",
    "\n",
    "for file in files: \n",
    "    f = open('training/pos/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    summary_list.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_tfidf = vectorizer.fit_transform(summary_list)\n",
    "tfidf_feature = train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vector(fv, n_c):\n",
    "    svd_transformer = TruncatedSVD(n_components=n_c, n_iter=7, random_state=42)\n",
    "    svd = svd_transformer.fit_transform(fv,Sentiment)\n",
    "    return svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_components = [300, 600, 900]\n",
    "Hidden_Layers2 = [(250,10),(600,10)]\n",
    "Activation_Func2 = ['relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 300 components:\n",
      "With hidden layer (250, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9309309309309308\n",
      "With hidden layer (600, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9419419419419419\n",
      "with 600 components:\n",
      "With hidden layer (250, 10), using activation function relu, the average accuracy to predict sentiment score is 0.924924924924925\n",
      "With hidden layer (600, 10), using activation function relu, the average accuracy to predict sentiment score is 0.92992992992993\n",
      "with 900 components:\n",
      "With hidden layer (250, 10), using activation function relu, the average accuracy to predict sentiment score is 0.937937937937938\n",
      "With hidden layer (600, 10), using activation function relu, the average accuracy to predict sentiment score is 0.9319319319319319\n"
     ]
    }
   ],
   "source": [
    "for h in number_of_components:\n",
    "    print('with '+str(h)+' components:')\n",
    "    for i in Hidden_Layers2:\n",
    "        for j in Activation_Func2:\n",
    "            tempX = list(svd_vector(tfidf_feature,h))\n",
    "            ten_fold_multilayer(tempX, Sentiment, i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3_4 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(600,10), activation = 'relu',random_state = 1)\n",
    "clf3_4.fit(list(svd_vector(tfidf_feature,300)), Sentiment)\n",
    "y_pred_3_4 = clf3_4.predict(list(svd_vector(tfidf_feature,300)))\n",
    "accuracy3_4 = accuracy_score(Sentiment, y_pred_3_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_transformer1 = TruncatedSVD(n_components=300, n_iter=7, random_state=42)\n",
    "svd1 = svd_transformer1.fit(tfidf_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = np.array(vectorizer.get_feature_names())\n",
    "weight = np.argsort(svd1.singular_values_).flatten()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "top_n = topic[weight][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000000', '00a', '00am'], dtype='<U32')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.73255211e-03,  1.32318325e-03,  5.91089788e-05, ...,\n",
       "         3.14515864e-05,  3.14515864e-05,  3.14515864e-05],\n",
       "       [ 1.14342345e-02,  1.61863908e-03, -7.61384861e-05, ...,\n",
       "        -2.65806449e-05, -2.65806449e-05, -2.65806449e-05],\n",
       "       [ 8.56233157e-03,  3.73887052e-03,  3.17068405e-05, ...,\n",
       "        -2.80741789e-04, -2.80741789e-04, -2.80741789e-04],\n",
       "       ...,\n",
       "       [ 5.44553562e-02,  1.60506269e-04, -7.72609053e-04, ...,\n",
       "        -9.88369769e-04, -9.88369769e-04, -9.88369769e-04],\n",
       "       [ 1.70943091e-02,  2.90847610e-03, -6.70595420e-04, ...,\n",
       "         1.29130783e-03,  1.29130783e-03,  1.29130783e-03],\n",
       "       [-1.58436542e-03, -9.30359719e-04, -2.70084618e-03, ...,\n",
       "         1.78089013e-03,  1.78089013e-03,  1.78089013e-03]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd1.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_test= os.listdir('test/') #get all file names under the training dataset\n",
    "\n",
    "for file in files_test: \n",
    "    f = open('test/'+file, mode = 'r')\n",
    "    article = f.read()\n",
    "    \n",
    "    f.close()\n",
    "    windows = tokenit(article.lower())\n",
    "    temp_feature = []\n",
    "    for i in VOC:\n",
    "        if i in windows:\n",
    "            temp_feature.append(1)\n",
    "        else:\n",
    "            temp_feature.append(0)\n",
    "    test_vector.append(temp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3_2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(250,10), activation = 'relu',random_state = 1)\n",
    "clf3_2.fit(words_count_feature, Sentiment)\n",
    "y_pred_3_6 = clf3_2.predict(test_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = []\n",
    "neg_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred_3_6)):\n",
    "    if y_pred_3_6[i] ==1:\n",
    "        neg_test.append(files_test[i])\n",
    "    if y_pred_3_6[i] ==0:\n",
    "        pos_test.append(files_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos.txt', 'w') as f:\n",
    "    for i in pos_test:\n",
    "        f.writelines(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('neg.txt', 'w') as f:\n",
    "    for i in neg_test:\n",
    "        f.writelines(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
